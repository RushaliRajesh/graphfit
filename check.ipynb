{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_no_noise.txt\n",
      "19 total pcds\n",
      "8.244805127117933\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_low_noise.txt\n",
      "19 total pcds\n",
      "10.21919190668316\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_med_noise.txt\n",
      "19 total pcds\n",
      "nan\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_high_noise.txt\n",
      "19 total pcds\n",
      "nan\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_gradient.txt\n",
      "16 total pcds\n",
      "nan\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_striped.txt\n",
      "16 total pcds\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1266109/2005758269.py:52: RuntimeWarning: Mean of empty slice.\n",
      "  print(np.array(rms_list).mean())\n"
     ]
    }
   ],
   "source": [
    "'''final wala'''\n",
    "import os\n",
    "from natsort import natsorted\n",
    "\n",
    "def rms_angular_error(estimated_normals, ground_truth_normals):\n",
    "    estimated_normals = F.normalize(estimated_normals, dim=1)\n",
    "    ground_truth_normals = F.normalize(ground_truth_normals, dim=1)\n",
    "\n",
    "    dot_product = torch.sum(estimated_normals * ground_truth_normals, dim=1)\n",
    "    dot_product = torch.clamp(dot_product, -1.0, 1.0)\n",
    "    dot_product = torch.abs(dot_product)\n",
    "    angular_diff = torch.acos(dot_product) * torch.div(180.0, torch.pi)\n",
    "    squared_diff = angular_diff.pow(2)\n",
    "    mean_squared_diff = torch.mean(squared_diff)\n",
    "    rms_angular_error = torch.sqrt(mean_squared_diff)\n",
    "    return rms_angular_error.item() \n",
    "\n",
    "pred_root = 'gap_try.py/GraphFit/results_599'\n",
    "gt_root = '/media/ashish/zoneD/AdaFit/data/pcpnet'\n",
    "\n",
    "test_Sets = ['/media/ashish/zoneD/AdaFit/data/pcpnet/testset_no_noise.txt', '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_low_noise.txt', \n",
    "             '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_med_noise.txt','/media/ashish/zoneD/AdaFit/data/pcpnet/testset_high_noise.txt', \n",
    "             '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_gradient.txt', '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_striped.txt']\n",
    "\n",
    "for testset in test_Sets:\n",
    "        rms_mean = 0\n",
    "        rms_list = []\n",
    "        shapes = []\n",
    "        #read files in pred_root\n",
    "\n",
    "        with open(testset, 'r') as f: \n",
    "                file_content = f.read()\n",
    "                words = file_content.split()\n",
    "                shapes.extend(words)\n",
    "                \n",
    "\n",
    "        for ind,shp in enumerate(shapes[:]):\n",
    "                #check if file exists\n",
    "                if (os.path.exists(os.path.join(pred_root, shp+'.normals')) and os.path.exists(os.path.join(gt_root, shp+'.normals'))):\n",
    "                        # print(os.path.join(pred_root, shp+'.normals'))\n",
    "                        try:\n",
    "                                pred = np.loadtxt(os.path.join(pred_root, shp+'.normals'), encoding='utf-8')\n",
    "                                # Process the loaded data here\n",
    "                        except UnicodeDecodeError:\n",
    "                                print(f\"Skipping file '{os.path.join(pred_root, shp+'.normals')}' due to UnicodeDecodeError.\")\n",
    "                                continue\n",
    "                        gt = np.loadtxt(os.path.join(gt_root, shp+'.normals'))\n",
    "                        # print(os.path.join(pred_root, shp+'.normals'), os.path.join(gt_root, shp+'.normals'))\n",
    "                        rms_list.append(rms_angular_error(torch.tensor(pred), torch.tensor(gt)))\n",
    "        print(testset)\n",
    "        print(ind+1, \"total pcds\")\n",
    "        print(np.array(rms_list).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_no_noise.txt\n",
      "19 total pcds\n",
      "10.366434734562919\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_low_noise.txt\n",
      "19 total pcds\n",
      "12.446189587383039\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_med_noise.txt\n",
      "19 total pcds\n",
      "nan\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_high_noise.txt\n",
      "19 total pcds\n",
      "nan\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_gradient.txt\n",
      "16 total pcds\n",
      "nan\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_striped.txt\n",
      "16 total pcds\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1266109/2449900698.py:52: RuntimeWarning: Mean of empty slice.\n",
      "  print(np.array(rms_list).mean())\n",
      "/home/ashish/anaconda3/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "'''final wala'''\n",
    "import os\n",
    "from natsort import natsorted\n",
    "\n",
    "def rms_angular_error(estimated_normals, ground_truth_normals):\n",
    "    estimated_normals = F.normalize(estimated_normals, dim=1)\n",
    "    ground_truth_normals = F.normalize(ground_truth_normals, dim=1)\n",
    "\n",
    "    dot_product = torch.sum(estimated_normals * ground_truth_normals, dim=1)\n",
    "    dot_product = torch.clamp(dot_product, -1.0, 1.0)\n",
    "    dot_product = torch.abs(dot_product)\n",
    "    angular_diff = torch.acos(dot_product) * torch.div(180.0, torch.pi)\n",
    "    squared_diff = angular_diff.pow(2)\n",
    "    mean_squared_diff = torch.mean(squared_diff)\n",
    "    rms_angular_error = torch.sqrt(mean_squared_diff)\n",
    "    return rms_angular_error.item() \n",
    "\n",
    "pred_root = 'gap_no_nlearn.py/GraphFit/results_599'\n",
    "gt_root = '/media/ashish/zoneD/AdaFit/data/pcpnet'\n",
    "\n",
    "test_Sets = ['/media/ashish/zoneD/AdaFit/data/pcpnet/testset_no_noise.txt', '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_low_noise.txt', \n",
    "             '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_med_noise.txt','/media/ashish/zoneD/AdaFit/data/pcpnet/testset_high_noise.txt', \n",
    "             '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_gradient.txt', '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_striped.txt']\n",
    "\n",
    "for testset in test_Sets:\n",
    "        rms_mean = 0\n",
    "        rms_list = []\n",
    "        shapes = []\n",
    "        #read files in pred_root\n",
    "\n",
    "        with open(testset, 'r') as f: \n",
    "                file_content = f.read()\n",
    "                words = file_content.split()\n",
    "                shapes.extend(words)\n",
    "                \n",
    "\n",
    "        for ind,shp in enumerate(shapes[:]):\n",
    "                #check if file exists\n",
    "                if (os.path.exists(os.path.join(pred_root, shp+'.normals')) and os.path.exists(os.path.join(gt_root, shp+'.normals'))):\n",
    "                        # print(os.path.join(pred_root, shp+'.normals'))\n",
    "                        try:\n",
    "                                pred = np.loadtxt(os.path.join(pred_root, shp+'.normals'), encoding='utf-8')\n",
    "                                # Process the loaded data here\n",
    "                        except UnicodeDecodeError:\n",
    "                                print(f\"Skipping file '{os.path.join(pred_root, shp+'.normals')}' due to UnicodeDecodeError.\")\n",
    "                                continue\n",
    "                        gt = np.loadtxt(os.path.join(gt_root, shp+'.normals'))\n",
    "                        # print(os.path.join(pred_root, shp+'.normals'), os.path.join(gt_root, shp+'.normals'))\n",
    "                        rms_list.append(rms_angular_error(torch.tensor(pred), torch.tensor(gt)))\n",
    "        print(testset)\n",
    "        print(ind+1, \"total pcds\")\n",
    "        print(np.array(rms_list).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_no_noise.txt\n",
      "19 total pcds\n",
      "5.600448071186137\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_low_noise.txt\n",
      "19 total pcds\n",
      "9.641333069792747\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_med_noise.txt\n",
      "19 total pcds\n",
      "17.63512195701019\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_high_noise.txt\n",
      "19 total pcds\n",
      "26.009528256709963\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_gradient.txt\n",
      "16 total pcds\n",
      "6.110188445452939\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_striped.txt\n",
      "16 total pcds\n",
      "6.4020866851712075\n"
     ]
    }
   ],
   "source": [
    "'''final wala'''\n",
    "import os\n",
    "from natsort import natsorted\n",
    "\n",
    "def rms_angular_error(estimated_normals, ground_truth_normals):\n",
    "    estimated_normals = F.normalize(estimated_normals, dim=1)\n",
    "    ground_truth_normals = F.normalize(ground_truth_normals, dim=1)\n",
    "\n",
    "    dot_product = torch.sum(estimated_normals * ground_truth_normals, dim=1)\n",
    "    dot_product = torch.clamp(dot_product, -1.0, 1.0)\n",
    "    dot_product = torch.abs(dot_product)\n",
    "    angular_diff = torch.acos(dot_product) * torch.div(180.0, torch.pi)\n",
    "    squared_diff = angular_diff.pow(2)\n",
    "    mean_squared_diff = torch.mean(squared_diff)\n",
    "    rms_angular_error = torch.sqrt(mean_squared_diff)\n",
    "    return rms_angular_error.item() \n",
    "\n",
    "pred_root = 'mh_of_4_22.py/GraphFit/results_599'\n",
    "gt_root = '/media/ashish/zoneD/AdaFit/data/pcpnet'\n",
    "\n",
    "test_Sets = ['/media/ashish/zoneD/AdaFit/data/pcpnet/testset_no_noise.txt', '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_low_noise.txt', \n",
    "             '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_med_noise.txt','/media/ashish/zoneD/AdaFit/data/pcpnet/testset_high_noise.txt', \n",
    "             '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_gradient.txt', '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_striped.txt']\n",
    "\n",
    "for testset in test_Sets:\n",
    "        rms_mean = 0\n",
    "        rms_list = []\n",
    "        shapes = []\n",
    "        #read files in pred_root\n",
    "\n",
    "        with open(testset, 'r') as f: \n",
    "                file_content = f.read()\n",
    "                words = file_content.split()\n",
    "                shapes.extend(words)\n",
    "                \n",
    "\n",
    "        for ind,shp in enumerate(shapes[:]):\n",
    "                #check if file exists\n",
    "                if (os.path.exists(os.path.join(pred_root, shp+'.normals')) and os.path.exists(os.path.join(gt_root, shp+'.normals'))):\n",
    "                        # print(os.path.join(pred_root, shp+'.normals'))\n",
    "                        try:\n",
    "                                pred = np.loadtxt(os.path.join(pred_root, shp+'.normals'), encoding='utf-8')\n",
    "                                # Process the loaded data here\n",
    "                        except UnicodeDecodeError:\n",
    "                                print(f\"Skipping file '{os.path.join(pred_root, shp+'.normals')}' due to UnicodeDecodeError.\")\n",
    "                                continue\n",
    "                        gt = np.loadtxt(os.path.join(gt_root, shp+'.normals'))\n",
    "                        # print(os.path.join(pred_root, shp+'.normals'), os.path.join(gt_root, shp+'.normals'))\n",
    "                        rms_list.append(rms_angular_error(torch.tensor(pred), torch.tensor(gt)))\n",
    "        print(testset)\n",
    "        print(ind+1, \"total pcds\")\n",
    "        print(np.array(rms_list).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_no_noise.txt\n",
      "19 total pcds\n",
      "5.339891393260973\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_low_noise.txt\n",
      "19 total pcds\n",
      "9.465132522102637\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_med_noise.txt\n",
      "19 total pcds\n",
      "17.595617176883053\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_high_noise.txt\n",
      "19 total pcds\n",
      "25.819010599450223\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_gradient.txt\n",
      "16 total pcds\n",
      "5.8555544234658985\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_striped.txt\n",
      "16 total pcds\n",
      "6.166880667544858\n"
     ]
    }
   ],
   "source": [
    "'''final wala'''\n",
    "import os\n",
    "from natsort import natsorted\n",
    "\n",
    "def rms_angular_error(estimated_normals, ground_truth_normals):\n",
    "    estimated_normals = F.normalize(estimated_normals, dim=1)\n",
    "    ground_truth_normals = F.normalize(ground_truth_normals, dim=1)\n",
    "\n",
    "    dot_product = torch.sum(estimated_normals * ground_truth_normals, dim=1)\n",
    "    dot_product = torch.clamp(dot_product, -1.0, 1.0)\n",
    "    dot_product = torch.abs(dot_product)\n",
    "    angular_diff = torch.acos(dot_product) * torch.div(180.0, torch.pi)\n",
    "    squared_diff = angular_diff.pow(2)\n",
    "    mean_squared_diff = torch.mean(squared_diff)\n",
    "    rms_angular_error = torch.sqrt(mean_squared_diff)\n",
    "    return rms_angular_error.item() \n",
    "\n",
    "pred_root = 'gat_knn/GraphFit/results_599'\n",
    "gt_root = '/media/ashish/zoneD/AdaFit/data/pcpnet'\n",
    "\n",
    "test_Sets = ['/media/ashish/zoneD/AdaFit/data/pcpnet/testset_no_noise.txt', '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_low_noise.txt', \n",
    "             '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_med_noise.txt','/media/ashish/zoneD/AdaFit/data/pcpnet/testset_high_noise.txt', \n",
    "             '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_gradient.txt', '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_striped.txt']\n",
    "\n",
    "for testset in test_Sets:\n",
    "        rms_mean = 0\n",
    "        rms_list = []\n",
    "        shapes = []\n",
    "        #read files in pred_root\n",
    "\n",
    "        with open(testset, 'r') as f: \n",
    "                file_content = f.read()\n",
    "                words = file_content.split()\n",
    "                shapes.extend(words)\n",
    "                \n",
    "\n",
    "        for ind,shp in enumerate(shapes[:]):\n",
    "                #check if file exists\n",
    "                if (os.path.exists(os.path.join(pred_root, shp+'.normals')) and os.path.exists(os.path.join(gt_root, shp+'.normals'))):\n",
    "                        # print(os.path.join(pred_root, shp+'.normals'))\n",
    "                        try:\n",
    "                                pred = np.loadtxt(os.path.join(pred_root, shp+'.normals'), encoding='utf-8')\n",
    "                                # Process the loaded data here\n",
    "                        except UnicodeDecodeError:\n",
    "                                print(f\"Skipping file '{os.path.join(pred_root, shp+'.normals')}' due to UnicodeDecodeError.\")\n",
    "                                continue\n",
    "                        gt = np.loadtxt(os.path.join(gt_root, shp+'.normals'))\n",
    "                        # print(os.path.join(pred_root, shp+'.normals'), os.path.join(gt_root, shp+'.normals'))\n",
    "                        rms_list.append(rms_angular_error(torch.tensor(pred), torch.tensor(gt)))\n",
    "        print(testset)\n",
    "        print(ind+1, \"total pcds\")\n",
    "        print(np.array(rms_list).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_no_noise.txt\n",
      "19 total pcds\n",
      "12.50633554607646\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_low_noise.txt\n",
      "19 total pcds\n",
      "12.956324836019693\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_med_noise.txt\n",
      "19 total pcds\n",
      "18.434769201300742\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_high_noise.txt\n",
      "19 total pcds\n",
      "26.90612213118597\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_gradient.txt\n",
      "16 total pcds\n",
      "13.153603781988673\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_striped.txt\n",
      "16 total pcds\n",
      "13.97335537340026\n"
     ]
    }
   ],
   "source": [
    "'''final wala'''\n",
    "import os\n",
    "from natsort import natsorted\n",
    "\n",
    "def rms_angular_error(estimated_normals, ground_truth_normals):\n",
    "    estimated_normals = F.normalize(estimated_normals, dim=1)\n",
    "    ground_truth_normals = F.normalize(ground_truth_normals, dim=1)\n",
    "\n",
    "    dot_product = torch.sum(estimated_normals * ground_truth_normals, dim=1)\n",
    "    dot_product = torch.clamp(dot_product, -1.0, 1.0)\n",
    "    dot_product = torch.abs(dot_product)\n",
    "    angular_diff = torch.acos(dot_product) * torch.div(180.0, torch.pi)\n",
    "    squared_diff = angular_diff.pow(2)\n",
    "    mean_squared_diff = torch.mean(squared_diff)\n",
    "    rms_angular_error = torch.sqrt(mean_squared_diff)\n",
    "    return rms_angular_error.item() \n",
    "\n",
    "pred_root = 'gat_based_lr/GraphFit/results_599'\n",
    "gt_root = '/media/ashish/zoneD/AdaFit/data/pcpnet'\n",
    "\n",
    "test_Sets = ['/media/ashish/zoneD/AdaFit/data/pcpnet/testset_no_noise.txt', '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_low_noise.txt', \n",
    "             '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_med_noise.txt','/media/ashish/zoneD/AdaFit/data/pcpnet/testset_high_noise.txt', \n",
    "             '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_gradient.txt', '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_striped.txt']\n",
    "\n",
    "for testset in test_Sets:\n",
    "        rms_mean = 0\n",
    "        rms_list = []\n",
    "        shapes = []\n",
    "        #read files in pred_root\n",
    "\n",
    "        with open(testset, 'r') as f: \n",
    "                file_content = f.read()\n",
    "                words = file_content.split()\n",
    "                shapes.extend(words)\n",
    "                \n",
    "\n",
    "        for ind,shp in enumerate(shapes[:]):\n",
    "                #check if file exists\n",
    "                if (os.path.exists(os.path.join(pred_root, shp+'.normals')) and os.path.exists(os.path.join(gt_root, shp+'.normals'))):\n",
    "                        # print(os.path.join(pred_root, shp+'.normals'))\n",
    "                        try:\n",
    "                                pred = np.loadtxt(os.path.join(pred_root, shp+'.normals'), encoding='utf-8')\n",
    "                                # Process the loaded data here\n",
    "                        except UnicodeDecodeError:\n",
    "                                print(f\"Skipping file '{os.path.join(pred_root, shp+'.normals')}' due to UnicodeDecodeError.\")\n",
    "                                continue\n",
    "                        gt = np.loadtxt(os.path.join(gt_root, shp+'.normals'))\n",
    "                        # print(os.path.join(pred_root, shp+'.normals'), os.path.join(gt_root, shp+'.normals'))\n",
    "                        rms_list.append(rms_angular_error(torch.tensor(pred), torch.tensor(gt)))\n",
    "        print(testset)\n",
    "        print(ind+1, \"total pcds\")\n",
    "        print(np.array(rms_list).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_no_noise.txt\n",
      "19 total pcds\n",
      "11.762748094904046\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_low_noise.txt\n",
      "19 total pcds\n",
      "nan\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_med_noise.txt\n",
      "19 total pcds\n",
      "nan\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_high_noise.txt\n",
      "19 total pcds\n",
      "nan\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_gradient.txt\n",
      "16 total pcds\n",
      "nan\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_striped.txt\n",
      "16 total pcds\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4110940/2576392270.py:52: RuntimeWarning: Mean of empty slice.\n",
      "  print(np.array(rms_list).mean())\n"
     ]
    }
   ],
   "source": [
    "'''final wala'''\n",
    "import os\n",
    "from natsort import natsorted\n",
    "\n",
    "def rms_angular_error(estimated_normals, ground_truth_normals):\n",
    "    estimated_normals = F.normalize(estimated_normals, dim=1)\n",
    "    ground_truth_normals = F.normalize(ground_truth_normals, dim=1)\n",
    "\n",
    "    dot_product = torch.sum(estimated_normals * ground_truth_normals, dim=1)\n",
    "    dot_product = torch.clamp(dot_product, -1.0, 1.0)\n",
    "    dot_product = torch.abs(dot_product)\n",
    "    angular_diff = torch.acos(dot_product) * torch.div(180.0, torch.pi)\n",
    "    squared_diff = angular_diff.pow(2)\n",
    "    mean_squared_diff = torch.mean(squared_diff)\n",
    "    rms_angular_error = torch.sqrt(mean_squared_diff)\n",
    "    return rms_angular_error.item() \n",
    "\n",
    "pred_root = 'gat_based/GraphFit/results_450'\n",
    "gt_root = '/media/ashish/zoneD/AdaFit/data/pcpnet'\n",
    "\n",
    "test_Sets = ['/media/ashish/zoneD/AdaFit/data/pcpnet/testset_no_noise.txt', '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_low_noise.txt', \n",
    "             '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_med_noise.txt','/media/ashish/zoneD/AdaFit/data/pcpnet/testset_high_noise.txt', \n",
    "             '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_gradient.txt', '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_striped.txt']\n",
    "\n",
    "for testset in test_Sets:\n",
    "        rms_mean = 0\n",
    "        rms_list = []\n",
    "        shapes = []\n",
    "        #read files in pred_root\n",
    "\n",
    "        with open(testset, 'r') as f: \n",
    "                file_content = f.read()\n",
    "                words = file_content.split()\n",
    "                shapes.extend(words)\n",
    "                \n",
    "\n",
    "        for ind,shp in enumerate(shapes[:]):\n",
    "                #check if file exists\n",
    "                if (os.path.exists(os.path.join(pred_root, shp+'.normals')) and os.path.exists(os.path.join(gt_root, shp+'.normals'))):\n",
    "                        # print(os.path.join(pred_root, shp+'.normals'))\n",
    "                        try:\n",
    "                                pred = np.loadtxt(os.path.join(pred_root, shp+'.normals'), encoding='utf-8')\n",
    "                                # Process the loaded data here\n",
    "                        except UnicodeDecodeError:\n",
    "                                print(f\"Skipping file '{os.path.join(pred_root, shp+'.normals')}' due to UnicodeDecodeError.\")\n",
    "                                continue\n",
    "                        gt = np.loadtxt(os.path.join(gt_root, shp+'.normals'))\n",
    "                        # print(os.path.join(pred_root, shp+'.normals'), os.path.join(gt_root, shp+'.normals'))\n",
    "                        rms_list.append(rms_angular_error(torch.tensor(pred), torch.tensor(gt)))\n",
    "        print(testset)\n",
    "        print(ind+1, \"total pcds\")\n",
    "        print(np.array(rms_list).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_no_noise.txt\n",
      "19 total pcds\n",
      "4.981540004547265\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_low_noise.txt\n",
      "19 total pcds\n",
      "9.722142634864412\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_med_noise.txt\n",
      "19 total pcds\n",
      "21.849054205418074\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_high_noise.txt\n",
      "19 total pcds\n",
      "nan\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_gradient.txt\n",
      "16 total pcds\n",
      "nan\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_striped.txt\n",
      "16 total pcds\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4110940/2330483852.py:52: RuntimeWarning: Mean of empty slice.\n",
      "  print(np.array(rms_list).mean())\n"
     ]
    }
   ],
   "source": [
    "'''final wala'''\n",
    "import os\n",
    "from natsort import natsorted\n",
    "\n",
    "def rms_angular_error(estimated_normals, ground_truth_normals):\n",
    "    estimated_normals = F.normalize(estimated_normals, dim=1)\n",
    "    ground_truth_normals = F.normalize(ground_truth_normals, dim=1)\n",
    "\n",
    "    dot_product = torch.sum(estimated_normals * ground_truth_normals, dim=1)\n",
    "    dot_product = torch.clamp(dot_product, -1.0, 1.0)\n",
    "    dot_product = torch.abs(dot_product)\n",
    "    angular_diff = torch.acos(dot_product) * torch.div(180.0, torch.pi)\n",
    "    squared_diff = angular_diff.pow(2)\n",
    "    mean_squared_diff = torch.mean(squared_diff)\n",
    "    rms_angular_error = torch.sqrt(mean_squared_diff)\n",
    "    return rms_angular_error.item() \n",
    "\n",
    "pred_root = 'k_100/GraphFit/results_599'\n",
    "gt_root = '/media/ashish/zoneD/AdaFit/data/pcpnet'\n",
    "\n",
    "test_Sets = ['/media/ashish/zoneD/AdaFit/data/pcpnet/testset_no_noise.txt', '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_low_noise.txt', \n",
    "             '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_med_noise.txt','/media/ashish/zoneD/AdaFit/data/pcpnet/testset_high_noise.txt', \n",
    "             '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_gradient.txt', '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_striped.txt']\n",
    "\n",
    "for testset in test_Sets:\n",
    "        rms_mean = 0\n",
    "        rms_list = []\n",
    "        shapes = []\n",
    "        #read files in pred_root\n",
    "\n",
    "        with open(testset, 'r') as f: \n",
    "                file_content = f.read()\n",
    "                words = file_content.split()\n",
    "                shapes.extend(words)\n",
    "                \n",
    "\n",
    "        for ind,shp in enumerate(shapes[:]):\n",
    "                #check if file exists\n",
    "                if (os.path.exists(os.path.join(pred_root, shp+'.normals')) and os.path.exists(os.path.join(gt_root, shp+'.normals'))):\n",
    "                        # print(os.path.join(pred_root, shp+'.normals'))\n",
    "                        try:\n",
    "                                pred = np.loadtxt(os.path.join(pred_root, shp+'.normals'), encoding='utf-8')\n",
    "                                # Process the loaded data here\n",
    "                        except UnicodeDecodeError:\n",
    "                                print(f\"Skipping file '{os.path.join(pred_root, shp+'.normals')}' due to UnicodeDecodeError.\")\n",
    "                                continue\n",
    "                        gt = np.loadtxt(os.path.join(gt_root, shp+'.normals'))\n",
    "                        # print(os.path.join(pred_root, shp+'.normals'), os.path.join(gt_root, shp+'.normals'))\n",
    "                        rms_list.append(rms_angular_error(torch.tensor(pred), torch.tensor(gt)))\n",
    "        print(testset)\n",
    "        print(ind+1, \"total pcds\")\n",
    "        print(np.array(rms_list).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_no_noise.txt\n",
      "19 total pcds\n",
      "6.583140078798321\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_low_noise.txt\n",
      "19 total pcds\n",
      "9.720792700862924\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_med_noise.txt\n",
      "19 total pcds\n",
      "17.51931585976425\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_high_noise.txt\n",
      "19 total pcds\n",
      "25.763835914806\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_gradient.txt\n",
      "16 total pcds\n",
      "6.974535065177864\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_striped.txt\n",
      "16 total pcds\n",
      "7.932214330342301\n"
     ]
    }
   ],
   "source": [
    "'''final wala'''\n",
    "import os\n",
    "from natsort import natsorted\n",
    "\n",
    "def rms_angular_error(estimated_normals, ground_truth_normals):\n",
    "    estimated_normals = F.normalize(estimated_normals, dim=1)\n",
    "    ground_truth_normals = F.normalize(ground_truth_normals, dim=1)\n",
    "\n",
    "    dot_product = torch.sum(estimated_normals * ground_truth_normals, dim=1)\n",
    "    dot_product = torch.clamp(dot_product, -1.0, 1.0)\n",
    "    dot_product = torch.abs(dot_product)\n",
    "    angular_diff = torch.acos(dot_product) * torch.div(180.0, torch.pi)\n",
    "    squared_diff = angular_diff.pow(2)\n",
    "    mean_squared_diff = torch.mean(squared_diff)\n",
    "    rms_angular_error = torch.sqrt(mean_squared_diff)\n",
    "    return rms_angular_error.item() \n",
    "\n",
    "pred_root = 'sa_crossatn/GraphFit/results_599'\n",
    "gt_root = '/media/ashish/zoneD/AdaFit/data/pcpnet'\n",
    "\n",
    "test_Sets = ['/media/ashish/zoneD/AdaFit/data/pcpnet/testset_no_noise.txt', '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_low_noise.txt', \n",
    "             '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_med_noise.txt','/media/ashish/zoneD/AdaFit/data/pcpnet/testset_high_noise.txt', \n",
    "             '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_gradient.txt', '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_striped.txt']\n",
    "\n",
    "for testset in test_Sets:\n",
    "        rms_mean = 0\n",
    "        rms_list = []\n",
    "        shapes = []\n",
    "        #read files in pred_root\n",
    "\n",
    "        with open(testset, 'r') as f: \n",
    "                file_content = f.read()\n",
    "                words = file_content.split()\n",
    "                shapes.extend(words)\n",
    "                \n",
    "\n",
    "        for ind,shp in enumerate(shapes[:]):\n",
    "                #check if file exists\n",
    "                if (os.path.exists(os.path.join(pred_root, shp+'.normals')) and os.path.exists(os.path.join(gt_root, shp+'.normals'))):\n",
    "                        # print(os.path.join(pred_root, shp+'.normals'))\n",
    "                        try:\n",
    "                                pred = np.loadtxt(os.path.join(pred_root, shp+'.normals'), encoding='utf-8')\n",
    "                                # Process the loaded data here\n",
    "                        except UnicodeDecodeError:\n",
    "                                print(f\"Skipping file '{os.path.join(pred_root, shp+'.normals')}' due to UnicodeDecodeError.\")\n",
    "                                continue\n",
    "                        gt = np.loadtxt(os.path.join(gt_root, shp+'.normals'))\n",
    "                        # print(os.path.join(pred_root, shp+'.normals'), os.path.join(gt_root, shp+'.normals'))\n",
    "                        rms_list.append(rms_angular_error(torch.tensor(pred), torch.tensor(gt)))\n",
    "        print(testset)\n",
    "        print(ind+1, \"total pcds\")\n",
    "        print(np.array(rms_list).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graphfit_enh_wb/GraphFit/results_599/galera100k.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/icosahedron100k.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/netsuke100k.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/Cup34100k.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/sphere100k.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/cylinder100k.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/star_smooth100k.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/star_halfsmooth100k.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/star_sharp100k.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/Liberty100k.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/boxunion2100k.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/pipe100k.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/pipe_curve100k.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/column100k.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/column_head100k.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/Boxy_smooth100k.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/sphere_analytic100k.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/cylinder_analytic100k.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/sheet_analytic100k.normals\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_no_noise.txt\n",
      "19 total pcds\n",
      "4.227261839964974\n",
      "graphfit_enh_wb/GraphFit/results_599/galera100k_noise_white_1.00e-02.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/icosahedron100k_noise_white_1.00e-02.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/netsuke100k_noise_white_1.00e-02.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/Cup34100k_noise_white_1.00e-02.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/sphere100k_noise_white_1.00e-02.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/cylinder100k_noise_white_1.00e-02.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/star_smooth100k_noise_white_1.00e-02.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/star_halfsmooth100k_noise_white_1.00e-02.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/star_sharp100k_noise_white_1.00e-02.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/Liberty100k_noise_white_1.00e-02.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/boxunion2100k_noise_white_1.00e-02.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/pipe100k_noise_white_1.00e-02.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/pipe_curve100k_noise_white_1.00e-02.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/column100k_noise_white_1.00e-02.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/column_head100k_noise_white_1.00e-02.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/Boxy_smooth100k_noise_white_1.00e-02.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/sphere_analytic100k_noise_white_1.00e-02.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/cylinder_analytic100k_noise_white_1.00e-02.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/sheet_analytic100k_noise_white_1.00e-02.normals\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_low_noise.txt\n",
      "19 total pcds\n",
      "9.167032567901614\n",
      "graphfit_enh_wb/GraphFit/results_599/galera100k_noise_white_5.00e-02.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/icosahedron100k_noise_white_5.00e-02.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/netsuke100k_noise_white_5.00e-02.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/Cup34100k_noise_white_5.00e-02.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/sphere100k_noise_white_5.00e-02.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/cylinder100k_noise_white_5.00e-02.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/star_smooth100k_noise_white_5.00e-02.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/star_halfsmooth100k_noise_white_5.00e-02.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/star_sharp100k_noise_white_5.00e-02.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/Liberty100k_noise_white_5.00e-02.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/boxunion2100k_noise_white_5.00e-02.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/pipe100k_noise_white_5.00e-02.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/pipe_curve100k_noise_white_5.00e-02.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/column100k_noise_white_5.00e-02.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/column_head100k_noise_white_5.00e-02.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/Boxy_smooth100k_noise_white_5.00e-02.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/sphere_analytic100k_noise_white_5.00e-02.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/cylinder_analytic100k_noise_white_5.00e-02.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/sheet_analytic100k_noise_white_5.00e-02.normals\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_med_noise.txt\n",
      "19 total pcds\n",
      "17.45660823201246\n",
      "graphfit_enh_wb/GraphFit/results_599/galera100k_noise_white_1.00e-01.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/icosahedron100k_noise_white_1.00e-01.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/netsuke100k_noise_white_1.00e-01.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/Cup34100k_noise_white_1.00e-01.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/sphere100k_noise_white_1.00e-01.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/cylinder100k_noise_white_1.00e-01.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/star_smooth100k_noise_white_1.00e-01.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/star_halfsmooth100k_noise_white_1.00e-01.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/star_sharp100k_noise_white_1.00e-01.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/Liberty100k_noise_white_1.00e-01.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/boxunion2100k_noise_white_1.00e-01.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/pipe100k_noise_white_1.00e-01.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/pipe_curve100k_noise_white_1.00e-01.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/column100k_noise_white_1.00e-01.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/column_head100k_noise_white_1.00e-01.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/Boxy_smooth100k_noise_white_1.00e-01.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/sphere_analytic100k_noise_white_1.00e-01.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/cylinder_analytic100k_noise_white_1.00e-01.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/sheet_analytic100k_noise_white_1.00e-01.normals\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_high_noise.txt\n",
      "19 total pcds\n",
      "25.571898298738677\n",
      "graphfit_enh_wb/GraphFit/results_599/galera100k_ddist_minmax.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/icosahedron100k_ddist_minmax.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/netsuke100k_ddist_minmax.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/Cup34100k_ddist_minmax.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/sphere100k_ddist_minmax.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/cylinder100k_ddist_minmax.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/star_smooth100k_ddist_minmax.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/star_halfsmooth100k_ddist_minmax.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/star_sharp100k_ddist_minmax.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/Liberty100k_ddist_minmax.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/boxunion2100k_ddist_minmax.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/pipe100k_ddist_minmax.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/pipe_curve100k_ddist_minmax.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/column100k_ddist_minmax.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/column_head100k_ddist_minmax.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/Boxy_smooth100k_ddist_minmax.normals\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_gradient.txt\n",
      "16 total pcds\n",
      "4.787642324601638\n",
      "graphfit_enh_wb/GraphFit/results_599/galera100k_ddist_minmax_layers.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/icosahedron100k_ddist_minmax_layers.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/netsuke100k_ddist_minmax_layers.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/Cup34100k_ddist_minmax_layers.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/sphere100k_ddist_minmax_layers.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/cylinder100k_ddist_minmax_layers.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/star_smooth100k_ddist_minmax_layers.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/star_halfsmooth100k_ddist_minmax_layers.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/star_sharp100k_ddist_minmax_layers.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/Liberty100k_ddist_minmax_layers.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/boxunion2100k_ddist_minmax_layers.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/pipe100k_ddist_minmax_layers.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/pipe_curve100k_ddist_minmax_layers.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/column100k_ddist_minmax_layers.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/column_head100k_ddist_minmax_layers.normals\n",
      "graphfit_enh_wb/GraphFit/results_599/Boxy_smooth100k_ddist_minmax_layers.normals\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_striped.txt\n",
      "16 total pcds\n",
      "5.0090556641565955\n"
     ]
    }
   ],
   "source": [
    "'''final wala'''\n",
    "import os\n",
    "from natsort import natsorted\n",
    "\n",
    "def rms_angular_error(estimated_normals, ground_truth_normals):\n",
    "    estimated_normals = F.normalize(estimated_normals, dim=1)\n",
    "    ground_truth_normals = F.normalize(ground_truth_normals, dim=1)\n",
    "\n",
    "    dot_product = torch.sum(estimated_normals * ground_truth_normals, dim=1)\n",
    "    dot_product = torch.clamp(dot_product, -1.0, 1.0)\n",
    "    dot_product = torch.abs(dot_product)\n",
    "    angular_diff = torch.acos(dot_product) * torch.div(180.0, torch.pi)\n",
    "    squared_diff = angular_diff.pow(2)\n",
    "    mean_squared_diff = torch.mean(squared_diff)\n",
    "    rms_angular_error = torch.sqrt(mean_squared_diff)\n",
    "    return rms_angular_error.item() \n",
    "\n",
    "pred_root = 'graphfit_enh_wb/GraphFit/results_599'\n",
    "gt_root = '/media/ashish/zoneD/AdaFit/data/pcpnet'\n",
    "\n",
    "test_Sets = ['/media/ashish/zoneD/AdaFit/data/pcpnet/testset_no_noise.txt', '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_low_noise.txt', \n",
    "             '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_med_noise.txt','/media/ashish/zoneD/AdaFit/data/pcpnet/testset_high_noise.txt', \n",
    "             '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_gradient.txt', '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_striped.txt']\n",
    "\n",
    "for testset in test_Sets:\n",
    "        rms_mean = 0\n",
    "        rms_list = []\n",
    "        shapes = []\n",
    "        #read files in pred_root\n",
    "\n",
    "        with open(testset, 'r') as f: \n",
    "                file_content = f.read()\n",
    "                words = file_content.split()\n",
    "                shapes.extend(words)\n",
    "                \n",
    "\n",
    "        for ind,shp in enumerate(shapes[:]):\n",
    "                #check if file exists\n",
    "                if (os.path.exists(os.path.join(pred_root, shp+'.normals')) and os.path.exists(os.path.join(gt_root, shp+'.normals'))):\n",
    "                        print(os.path.join(pred_root, shp+'.normals'))\n",
    "                        try:\n",
    "                                pred = np.loadtxt(os.path.join(pred_root, shp+'.normals'), encoding='utf-8')\n",
    "                                # Process the loaded data here\n",
    "                        except UnicodeDecodeError:\n",
    "                                print(f\"Skipping file '{os.path.join(pred_root, shp+'.normals')}' due to UnicodeDecodeError.\")\n",
    "                                continue\n",
    "                        gt = np.loadtxt(os.path.join(gt_root, shp+'.normals'))\n",
    "                        # print(os.path.join(pred_root, shp+'.normals'), os.path.join(gt_root, shp+'.normals'))\n",
    "                        rms_list.append(rms_angular_error(torch.tensor(pred), torch.tensor(gt)))\n",
    "        print(testset)\n",
    "        print(ind+1, \"total pcds\")\n",
    "        print(np.array(rms_list).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_no_noise.txt\n",
      "19 total pcds\n",
      "4.227261839964974\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_low_noise.txt\n",
      "19 total pcds\n",
      "9.167032567901614\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_med_noise.txt\n",
      "19 total pcds\n",
      "17.45660823201246\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_high_noise.txt\n",
      "19 total pcds\n",
      "25.571898298738677\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_gradient.txt\n",
      "16 total pcds\n",
      "4.787642324601638\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_striped.txt\n",
      "16 total pcds\n",
      "5.0090556641565955\n"
     ]
    }
   ],
   "source": [
    "'''final wala'''\n",
    "import os\n",
    "from natsort import natsorted\n",
    "\n",
    "def rms_angular_error(estimated_normals, ground_truth_normals):\n",
    "    estimated_normals = F.normalize(estimated_normals, dim=1)\n",
    "    ground_truth_normals = F.normalize(ground_truth_normals, dim=1)\n",
    "\n",
    "    dot_product = torch.sum(estimated_normals * ground_truth_normals, dim=1)\n",
    "    dot_product = torch.clamp(dot_product, -1.0, 1.0)\n",
    "    dot_product = torch.abs(dot_product)\n",
    "    angular_diff = torch.acos(dot_product) * torch.div(180.0, torch.pi)\n",
    "    squared_diff = angular_diff.pow(2)\n",
    "    mean_squared_diff = torch.mean(squared_diff)\n",
    "    rms_angular_error = torch.sqrt(mean_squared_diff)\n",
    "    return rms_angular_error.item() \n",
    "\n",
    "pred_root = 'graphfit_enh_wb/GraphFit/results_2_599'\n",
    "gt_root = '/media/ashish/zoneD/AdaFit/data/pcpnet'\n",
    "\n",
    "test_Sets = ['/media/ashish/zoneD/AdaFit/data/pcpnet/testset_no_noise.txt', '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_low_noise.txt', \n",
    "             '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_med_noise.txt','/media/ashish/zoneD/AdaFit/data/pcpnet/testset_high_noise.txt', \n",
    "             '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_gradient.txt', '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_striped.txt']\n",
    "\n",
    "for testset in test_Sets:\n",
    "        rms_mean = 0\n",
    "        rms_list = []\n",
    "        shapes = []\n",
    "        #read files in pred_root\n",
    "\n",
    "        with open(testset, 'r') as f: \n",
    "                file_content = f.read()\n",
    "                words = file_content.split()\n",
    "                shapes.extend(words)\n",
    "                \n",
    "\n",
    "        for ind,shp in enumerate(shapes[:]):\n",
    "                #check if file exists\n",
    "                if (os.path.exists(os.path.join(pred_root, shp+'.normals')) and os.path.exists(os.path.join(gt_root, shp+'.normals'))):\n",
    "                        # print(os.path.join(pred_root, shp+'.normals'))\n",
    "                        try:\n",
    "                                pred = np.loadtxt(os.path.join(pred_root, shp+'.normals'), encoding='utf-8')\n",
    "                                # Process the loaded data here\n",
    "                        except UnicodeDecodeError:\n",
    "                                print(f\"Skipping file '{os.path.join(pred_root, shp+'.normals')}' due to UnicodeDecodeError.\")\n",
    "                                continue\n",
    "                        gt = np.loadtxt(os.path.join(gt_root, shp+'.normals'))\n",
    "                        # print(os.path.join(pred_root, shp+'.normals'), os.path.join(gt_root, shp+'.normals'))\n",
    "                        rms_list.append(rms_angular_error(torch.tensor(pred), torch.tensor(gt)))\n",
    "        print(testset)\n",
    "        print(ind+1, \"total pcds\")\n",
    "        print(np.array(rms_list).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_no_noise.txt\n",
      "19 total pcds\n",
      "4.639185351812269\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_low_noise.txt\n",
      "19 total pcds\n",
      "9.22376248223953\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_med_noise.txt\n",
      "19 total pcds\n",
      "21.4529356036846\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_high_noise.txt\n",
      "19 total pcds\n",
      "nan\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_gradient.txt\n",
      "16 total pcds\n",
      "nan\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_striped.txt\n",
      "16 total pcds\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4110940/2367242508.py:52: RuntimeWarning: Mean of empty slice.\n",
      "  print(np.array(rms_list).mean())\n"
     ]
    }
   ],
   "source": [
    "'''final wala'''\n",
    "import os\n",
    "from natsort import natsorted\n",
    "\n",
    "def rms_angular_error(estimated_normals, ground_truth_normals):\n",
    "    estimated_normals = F.normalize(estimated_normals, dim=1)\n",
    "    ground_truth_normals = F.normalize(ground_truth_normals, dim=1)\n",
    "\n",
    "    dot_product = torch.sum(estimated_normals * ground_truth_normals, dim=1)\n",
    "    dot_product = torch.clamp(dot_product, -1.0, 1.0)\n",
    "    dot_product = torch.abs(dot_product)\n",
    "    angular_diff = torch.acos(dot_product) * torch.div(180.0, torch.pi)\n",
    "    squared_diff = angular_diff.pow(2)\n",
    "    mean_squared_diff = torch.mean(squared_diff)\n",
    "    rms_angular_error = torch.sqrt(mean_squared_diff)\n",
    "    return rms_angular_error.item() \n",
    "\n",
    "pred_root = 'cor_sa_only1/GraphFit/results_599'\n",
    "gt_root = '/media/ashish/zoneD/AdaFit/data/pcpnet'\n",
    "\n",
    "test_Sets = ['/media/ashish/zoneD/AdaFit/data/pcpnet/testset_no_noise.txt', '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_low_noise.txt', \n",
    "             '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_med_noise.txt','/media/ashish/zoneD/AdaFit/data/pcpnet/testset_high_noise.txt', \n",
    "             '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_gradient.txt', '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_striped.txt']\n",
    "\n",
    "for testset in test_Sets:\n",
    "        rms_mean = 0\n",
    "        rms_list = []\n",
    "        shapes = []\n",
    "        #read files in pred_root\n",
    "\n",
    "        with open(testset, 'r') as f: \n",
    "                file_content = f.read()\n",
    "                words = file_content.split()\n",
    "                shapes.extend(words)\n",
    "                \n",
    "\n",
    "        for ind,shp in enumerate(shapes[:]):\n",
    "                #check if file exists\n",
    "                if (os.path.exists(os.path.join(pred_root, shp+'.normals')) and os.path.exists(os.path.join(gt_root, shp+'.normals'))):\n",
    "                        # print(os.path.join(pred_root, shp+'.normals'))\n",
    "                        try:\n",
    "                                pred = np.loadtxt(os.path.join(pred_root, shp+'.normals'), encoding='utf-8')\n",
    "                                # Process the loaded data here\n",
    "                        except UnicodeDecodeError:\n",
    "                                print(f\"Skipping file '{os.path.join(pred_root, shp+'.normals')}' due to UnicodeDecodeError.\")\n",
    "                                continue\n",
    "                        gt = np.loadtxt(os.path.join(gt_root, shp+'.normals'))\n",
    "                        # print(os.path.join(pred_root, shp+'.normals'), os.path.join(gt_root, shp+'.normals'))\n",
    "                        rms_list.append(rms_angular_error(torch.tensor(pred), torch.tensor(gt)))\n",
    "        print(testset)\n",
    "        print(ind+1, \"total pcds\")\n",
    "        print(np.array(rms_list).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_no_noise.txt\n",
      "19 total pcds\n",
      "4.929840791802344\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_low_noise.txt\n",
      "19 total pcds\n",
      "12.332908322413013\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_med_noise.txt\n",
      "19 total pcds\n",
      "nan\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_high_noise.txt\n",
      "19 total pcds\n",
      "nan\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_gradient.txt\n",
      "16 total pcds\n",
      "nan\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_striped.txt\n",
      "16 total pcds\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4110940/241541680.py:52: RuntimeWarning: Mean of empty slice.\n",
      "  print(np.array(rms_list).mean())\n"
     ]
    }
   ],
   "source": [
    "'''final wala'''\n",
    "import os\n",
    "from natsort import natsorted\n",
    "\n",
    "def rms_angular_error(estimated_normals, ground_truth_normals):\n",
    "    estimated_normals = F.normalize(estimated_normals, dim=1)\n",
    "    ground_truth_normals = F.normalize(ground_truth_normals, dim=1)\n",
    "\n",
    "    dot_product = torch.sum(estimated_normals * ground_truth_normals, dim=1)\n",
    "    dot_product = torch.clamp(dot_product, -1.0, 1.0)\n",
    "    dot_product = torch.abs(dot_product)\n",
    "    angular_diff = torch.acos(dot_product) * torch.div(180.0, torch.pi)\n",
    "    squared_diff = angular_diff.pow(2)\n",
    "    mean_squared_diff = torch.mean(squared_diff)\n",
    "    rms_angular_error = torch.sqrt(mean_squared_diff)\n",
    "    return rms_angular_error.item() \n",
    "\n",
    "pred_root = 'cor_mh/GraphFit/results_599'\n",
    "gt_root = '/media/ashish/zoneD/AdaFit/data/pcpnet'\n",
    "\n",
    "test_Sets = ['/media/ashish/zoneD/AdaFit/data/pcpnet/testset_no_noise.txt', '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_low_noise.txt', \n",
    "             '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_med_noise.txt','/media/ashish/zoneD/AdaFit/data/pcpnet/testset_high_noise.txt', \n",
    "             '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_gradient.txt', '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_striped.txt']\n",
    "\n",
    "for testset in test_Sets:\n",
    "        rms_mean = 0\n",
    "        rms_list = []\n",
    "        shapes = []\n",
    "        #read files in pred_root\n",
    "\n",
    "        with open(testset, 'r') as f: \n",
    "                file_content = f.read()\n",
    "                words = file_content.split()\n",
    "                shapes.extend(words)\n",
    "                \n",
    "\n",
    "        for ind,shp in enumerate(shapes[:]):\n",
    "                #check if file exists\n",
    "                if (os.path.exists(os.path.join(pred_root, shp+'.normals')) and os.path.exists(os.path.join(gt_root, shp+'.normals'))):\n",
    "                        # print(os.path.join(pred_root, shp+'.normals'))\n",
    "                        try:\n",
    "                                pred = np.loadtxt(os.path.join(pred_root, shp+'.normals'), encoding='utf-8')\n",
    "                                # Process the loaded data here\n",
    "                        except UnicodeDecodeError:\n",
    "                                print(f\"Skipping file '{os.path.join(pred_root, shp+'.normals')}' due to UnicodeDecodeError.\")\n",
    "                                continue\n",
    "                        gt = np.loadtxt(os.path.join(gt_root, shp+'.normals'))\n",
    "                        # print(os.path.join(pred_root, shp+'.normals'), os.path.join(gt_root, shp+'.normals'))\n",
    "                        rms_list.append(rms_angular_error(torch.tensor(pred), torch.tensor(gt)))\n",
    "        print(testset)\n",
    "        print(ind+1, \"total pcds\")\n",
    "        print(np.array(rms_list).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_no_noise.txt\n",
      "19 total pcds\n",
      "5.4203958697823875\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_low_noise.txt\n",
      "19 total pcds\n",
      "nan\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_med_noise.txt\n",
      "19 total pcds\n",
      "nan\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_high_noise.txt\n",
      "19 total pcds\n",
      "nan\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_gradient.txt\n",
      "16 total pcds\n",
      "nan\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_striped.txt\n",
      "16 total pcds\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4110940/3836147971.py:52: RuntimeWarning: Mean of empty slice.\n",
      "  print(np.array(rms_list).mean())\n"
     ]
    }
   ],
   "source": [
    "'''final wala'''\n",
    "import os\n",
    "from natsort import natsorted\n",
    "\n",
    "def rms_angular_error(estimated_normals, ground_truth_normals):\n",
    "    estimated_normals = F.normalize(estimated_normals, dim=1)\n",
    "    ground_truth_normals = F.normalize(ground_truth_normals, dim=1)\n",
    "\n",
    "    dot_product = torch.sum(estimated_normals * ground_truth_normals, dim=1)\n",
    "    dot_product = torch.clamp(dot_product, -1.0, 1.0)\n",
    "    dot_product = torch.abs(dot_product)\n",
    "    angular_diff = torch.acos(dot_product) * torch.div(180.0, torch.pi)\n",
    "    squared_diff = angular_diff.pow(2)\n",
    "    mean_squared_diff = torch.mean(squared_diff)\n",
    "    rms_angular_error = torch.sqrt(mean_squared_diff)\n",
    "    return rms_angular_error.item() \n",
    "\n",
    "pred_root = 'cor_sa/GraphFit/results_599'\n",
    "gt_root = '/media/ashish/zoneD/AdaFit/data/pcpnet'\n",
    "\n",
    "test_Sets = ['/media/ashish/zoneD/AdaFit/data/pcpnet/testset_no_noise.txt', '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_low_noise.txt', \n",
    "             '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_med_noise.txt','/media/ashish/zoneD/AdaFit/data/pcpnet/testset_high_noise.txt', \n",
    "             '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_gradient.txt', '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_striped.txt']\n",
    "\n",
    "for testset in test_Sets:\n",
    "        rms_mean = 0\n",
    "        rms_list = []\n",
    "        shapes = []\n",
    "        #read files in pred_root\n",
    "\n",
    "        with open(testset, 'r') as f: \n",
    "                file_content = f.read()\n",
    "                words = file_content.split()\n",
    "                shapes.extend(words)\n",
    "                \n",
    "\n",
    "        for ind,shp in enumerate(shapes[:]):\n",
    "                #check if file exists\n",
    "                if (os.path.exists(os.path.join(pred_root, shp+'.normals')) and os.path.exists(os.path.join(gt_root, shp+'.normals'))):\n",
    "                        # print(os.path.join(pred_root, shp+'.normals'))\n",
    "                        try:\n",
    "                                pred = np.loadtxt(os.path.join(pred_root, shp+'.normals'), encoding='utf-8')\n",
    "                                # Process the loaded data here\n",
    "                        except UnicodeDecodeError:\n",
    "                                print(f\"Skipping file '{os.path.join(pred_root, shp+'.normals')}' due to UnicodeDecodeError.\")\n",
    "                                continue\n",
    "                        gt = np.loadtxt(os.path.join(gt_root, shp+'.normals'))\n",
    "                        # print(os.path.join(pred_root, shp+'.normals'), os.path.join(gt_root, shp+'.normals'))\n",
    "                        rms_list.append(rms_angular_error(torch.tensor(pred), torch.tensor(gt)))\n",
    "        print(testset)\n",
    "        print(ind+1, \"total pcds\")\n",
    "        print(np.array(rms_list).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_no_noise.txt\n",
      "19 total pcds\n",
      "4.227261839964974\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_low_noise.txt\n",
      "19 total pcds\n",
      "9.167032567901614\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_med_noise.txt\n",
      "19 total pcds\n",
      "17.45660823201246\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_high_noise.txt\n",
      "19 total pcds\n",
      "25.571898298738677\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_gradient.txt\n",
      "16 total pcds\n",
      "4.787642324601638\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_striped.txt\n",
      "16 total pcds\n",
      "5.0090556641565955\n"
     ]
    }
   ],
   "source": [
    "'''final wala'''\n",
    "import os\n",
    "from natsort import natsorted\n",
    "\n",
    "def rms_angular_error(estimated_normals, ground_truth_normals):\n",
    "    estimated_normals = F.normalize(estimated_normals, dim=1)\n",
    "    ground_truth_normals = F.normalize(ground_truth_normals, dim=1)\n",
    "\n",
    "    dot_product = torch.sum(estimated_normals * ground_truth_normals, dim=1)\n",
    "    dot_product = torch.clamp(dot_product, -1.0, 1.0)\n",
    "    dot_product = torch.abs(dot_product)\n",
    "    angular_diff = torch.acos(dot_product) * torch.div(180.0, torch.pi)\n",
    "    squared_diff = angular_diff.pow(2)\n",
    "    mean_squared_diff = torch.mean(squared_diff)\n",
    "    rms_angular_error = torch.sqrt(mean_squared_diff)\n",
    "    return rms_angular_error.item() \n",
    "\n",
    "pred_root = 'graphfit_enh_wb/GraphFit/results_599'\n",
    "gt_root = '/media/ashish/zoneD/AdaFit/data/pcpnet'\n",
    "\n",
    "test_Sets = ['/media/ashish/zoneD/AdaFit/data/pcpnet/testset_no_noise.txt', '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_low_noise.txt', \n",
    "             '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_med_noise.txt','/media/ashish/zoneD/AdaFit/data/pcpnet/testset_high_noise.txt', \n",
    "             '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_gradient.txt', '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_striped.txt']\n",
    "\n",
    "for testset in test_Sets:\n",
    "        rms_mean = 0\n",
    "        rms_list = []\n",
    "        shapes = []\n",
    "        #read files in pred_root\n",
    "\n",
    "        with open(testset, 'r') as f: \n",
    "                file_content = f.read()\n",
    "                words = file_content.split()\n",
    "                shapes.extend(words)\n",
    "                \n",
    "\n",
    "        for ind,shp in enumerate(shapes[:]):\n",
    "                #check if file exists\n",
    "                if (os.path.exists(os.path.join(pred_root, shp+'.normals')) and os.path.exists(os.path.join(gt_root, shp+'.normals'))):\n",
    "                        # print(os.path.join(pred_root, shp+'.normals'))\n",
    "                        try:\n",
    "                                pred = np.loadtxt(os.path.join(pred_root, shp+'.normals'), encoding='utf-8')\n",
    "                                # Process the loaded data here\n",
    "                        except UnicodeDecodeError:\n",
    "                                print(f\"Skipping file '{os.path.join(pred_root, shp+'.normals')}' due to UnicodeDecodeError.\")\n",
    "                                continue\n",
    "                        gt = np.loadtxt(os.path.join(gt_root, shp+'.normals'))\n",
    "                        # print(os.path.join(pred_root, shp+'.normals'), os.path.join(gt_root, shp+'.normals'))\n",
    "                        rms_list.append(rms_angular_error(torch.tensor(pred), torch.tensor(gt)))\n",
    "        print(testset)\n",
    "        print(ind+1, \"total pcds\")\n",
    "        print(np.array(rms_list).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_no_noise.txt\n",
      "19 total pcds\n",
      "4.227261839964974\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_low_noise.txt\n",
      "19 total pcds\n",
      "9.167032567901614\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_med_noise.txt\n",
      "19 total pcds\n",
      "17.45660823201246\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_high_noise.txt\n",
      "19 total pcds\n",
      "25.571898298738677\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_gradient.txt\n",
      "16 total pcds\n",
      "4.787642324601638\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_striped.txt\n",
      "16 total pcds\n",
      "3.9708950601438957\n"
     ]
    }
   ],
   "source": [
    "'''final wala'''\n",
    "import os\n",
    "from natsort import natsorted\n",
    "\n",
    "def rms_angular_error(estimated_normals, ground_truth_normals):\n",
    "    estimated_normals = F.normalize(estimated_normals, dim=1)\n",
    "    ground_truth_normals = F.normalize(ground_truth_normals, dim=1)\n",
    "\n",
    "    dot_product = torch.sum(estimated_normals * ground_truth_normals, dim=1)\n",
    "    dot_product = torch.clamp(dot_product, -1.0, 1.0)\n",
    "    dot_product = torch.abs(dot_product)\n",
    "    angular_diff = torch.acos(dot_product) * torch.div(180.0, torch.pi)\n",
    "    squared_diff = angular_diff.pow(2)\n",
    "    mean_squared_diff = torch.mean(squared_diff)\n",
    "    rms_angular_error = torch.sqrt(mean_squared_diff)\n",
    "    return rms_angular_error.item() \n",
    "\n",
    "pred_root = 'graphfit_enh_wb/GraphFit/results_599'\n",
    "gt_root = '/media/ashish/zoneD/AdaFit/data/pcpnet'\n",
    "\n",
    "test_Sets = ['/media/ashish/zoneD/AdaFit/data/pcpnet/testset_no_noise.txt', '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_low_noise.txt', \n",
    "             '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_med_noise.txt','/media/ashish/zoneD/AdaFit/data/pcpnet/testset_high_noise.txt', \n",
    "             '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_gradient.txt', '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_striped.txt']\n",
    "\n",
    "for testset in test_Sets:\n",
    "        rms_mean = 0\n",
    "        rms_list = []\n",
    "        shapes = []\n",
    "        #read files in pred_root\n",
    "\n",
    "        with open(testset, 'r') as f: \n",
    "                file_content = f.read()\n",
    "                words = file_content.split()\n",
    "                shapes.extend(words)\n",
    "                \n",
    "\n",
    "        for ind,shp in enumerate(shapes[:]):\n",
    "                #check if file exists\n",
    "                if (os.path.exists(os.path.join(pred_root, shp+'.normals')) and os.path.exists(os.path.join(gt_root, shp+'.normals'))):\n",
    "                        # print(os.path.join(pred_root, shp+'.normals'))\n",
    "                        try:\n",
    "                                pred = np.loadtxt(os.path.join(pred_root, shp+'.normals'), encoding='utf-8')\n",
    "                                # Process the loaded data here\n",
    "                        except UnicodeDecodeError:\n",
    "                                print(f\"Skipping file '{os.path.join(pred_root, shp+'.normals')}' due to UnicodeDecodeError.\")\n",
    "                                continue\n",
    "                        gt = np.loadtxt(os.path.join(gt_root, shp+'.normals'))\n",
    "                        # print(os.path.join(pred_root, shp+'.normals'), os.path.join(gt_root, shp+'.normals'))\n",
    "                        rms_list.append(rms_angular_error(torch.tensor(pred), torch.tensor(gt)))\n",
    "        print(testset)\n",
    "        print(ind+1, \"total pcds\")\n",
    "        print(np.array(rms_list).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_no_noise.txt\n",
      "19 total pcds\n",
      "4.622787377367182\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_low_noise.txt\n",
      "19 total pcds\n",
      "9.137373993628263\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_med_noise.txt\n",
      "19 total pcds\n",
      "17.479394506724898\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_high_noise.txt\n",
      "19 total pcds\n",
      "25.58371796415752\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_gradient.txt\n",
      "16 total pcds\n",
      "5.104981381689026\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_striped.txt\n",
      "16 total pcds\n",
      "6.504110469085447\n"
     ]
    }
   ],
   "source": [
    "'''final wala'''\n",
    "import os\n",
    "from natsort import natsorted\n",
    "\n",
    "def rms_angular_error(estimated_normals, ground_truth_normals):\n",
    "    estimated_normals = F.normalize(estimated_normals, dim=1)\n",
    "    ground_truth_normals = F.normalize(ground_truth_normals, dim=1)\n",
    "\n",
    "    dot_product = torch.sum(estimated_normals * ground_truth_normals, dim=1)\n",
    "    dot_product = torch.clamp(dot_product, -1.0, 1.0)\n",
    "    dot_product = torch.abs(dot_product)\n",
    "    angular_diff = torch.acos(dot_product) * torch.div(180.0, torch.pi)\n",
    "    squared_diff = angular_diff.pow(2)\n",
    "    mean_squared_diff = torch.mean(squared_diff)\n",
    "    rms_angular_error = torch.sqrt(mean_squared_diff)\n",
    "    return rms_angular_error.item() \n",
    "\n",
    "pred_root = 'enh_nowb/GraphFit/results_599'\n",
    "gt_root = '/media/ashish/zoneD/AdaFit/data/pcpnet'\n",
    "\n",
    "test_Sets = ['/media/ashish/zoneD/AdaFit/data/pcpnet/testset_no_noise.txt', '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_low_noise.txt', \n",
    "             '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_med_noise.txt','/media/ashish/zoneD/AdaFit/data/pcpnet/testset_high_noise.txt', \n",
    "             '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_gradient.txt', '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_striped.txt']\n",
    "\n",
    "for testset in test_Sets:\n",
    "        rms_mean = 0\n",
    "        rms_list = []\n",
    "        shapes = []\n",
    "        #read files in pred_root\n",
    "\n",
    "        with open(testset, 'r') as f: \n",
    "                file_content = f.read()\n",
    "                words = file_content.split()\n",
    "                shapes.extend(words)\n",
    "                \n",
    "\n",
    "        for ind,shp in enumerate(shapes[:]):\n",
    "                #check if file exists\n",
    "                if (os.path.exists(os.path.join(pred_root, shp+'.normals')) and os.path.exists(os.path.join(gt_root, shp+'.normals'))):\n",
    "                        # print(os.path.join(pred_root, shp+'.normals'))\n",
    "                        try:\n",
    "                                pred = np.loadtxt(os.path.join(pred_root, shp+'.normals'), encoding='utf-8')\n",
    "                                # Process the loaded data here\n",
    "                        except UnicodeDecodeError:\n",
    "                                print(f\"Skipping file '{os.path.join(pred_root, shp+'.normals')}' due to UnicodeDecodeError.\")\n",
    "                                continue\n",
    "                        gt = np.loadtxt(os.path.join(gt_root, shp+'.normals'))\n",
    "                        # print(os.path.join(pred_root, shp+'.normals'), os.path.join(gt_root, shp+'.normals'))\n",
    "                        rms_list.append(rms_angular_error(torch.tensor(pred), torch.tensor(gt)))\n",
    "        print(testset)\n",
    "        print(ind+1, \"total pcds\")\n",
    "        print(np.array(rms_list).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_no_noise.txt\n",
      "19 total pcds\n",
      "5.360429707192586\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_low_noise.txt\n",
      "19 total pcds\n",
      "nan\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_med_noise.txt\n",
      "19 total pcds\n",
      "nan\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_high_noise.txt\n",
      "19 total pcds\n",
      "nan\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_gradient.txt\n",
      "16 total pcds\n",
      "nan\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_striped.txt\n",
      "16 total pcds\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_541302/2895995625.py:52: RuntimeWarning: Mean of empty slice.\n",
      "  print(np.array(rms_list).mean())\n"
     ]
    }
   ],
   "source": [
    "'''final wala'''\n",
    "import os\n",
    "from natsort import natsorted\n",
    "\n",
    "def rms_angular_error(estimated_normals, ground_truth_normals):\n",
    "    estimated_normals = F.normalize(estimated_normals, dim=1)\n",
    "    ground_truth_normals = F.normalize(ground_truth_normals, dim=1)\n",
    "\n",
    "    dot_product = torch.sum(estimated_normals * ground_truth_normals, dim=1)\n",
    "    dot_product = torch.clamp(dot_product, -1.0, 1.0)\n",
    "    dot_product = torch.abs(dot_product)\n",
    "    angular_diff = torch.acos(dot_product) * torch.div(180.0, torch.pi)\n",
    "    squared_diff = angular_diff.pow(2)\n",
    "    mean_squared_diff = torch.mean(squared_diff)\n",
    "    rms_angular_error = torch.sqrt(mean_squared_diff)\n",
    "    return rms_angular_error.item() \n",
    "\n",
    "pred_root = 'graphfit_enh/GraphFit/results_599'\n",
    "gt_root = '/media/ashish/zoneD/AdaFit/data/pcpnet'\n",
    "\n",
    "test_Sets = ['/media/ashish/zoneD/AdaFit/data/pcpnet/testset_no_noise.txt', '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_low_noise.txt', \n",
    "             '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_med_noise.txt','/media/ashish/zoneD/AdaFit/data/pcpnet/testset_high_noise.txt', \n",
    "             '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_gradient.txt', '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_striped.txt']\n",
    "\n",
    "for testset in test_Sets:\n",
    "        rms_mean = 0\n",
    "        rms_list = []\n",
    "        shapes = []\n",
    "        #read files in pred_root\n",
    "\n",
    "        with open(testset, 'r') as f: \n",
    "                file_content = f.read()\n",
    "                words = file_content.split()\n",
    "                shapes.extend(words)\n",
    "                \n",
    "\n",
    "        for ind,shp in enumerate(shapes[:]):\n",
    "                #check if file exists\n",
    "                if (os.path.exists(os.path.join(pred_root, shp+'.normals')) and os.path.exists(os.path.join(gt_root, shp+'.normals'))):\n",
    "                        # print(os.path.join(pred_root, shp+'.normals'))\n",
    "                        try:\n",
    "                                pred = np.loadtxt(os.path.join(pred_root, shp+'.normals'), encoding='utf-8')\n",
    "                                # Process the loaded data here\n",
    "                        except UnicodeDecodeError:\n",
    "                                print(f\"Skipping file '{os.path.join(pred_root, shp+'.normals')}' due to UnicodeDecodeError.\")\n",
    "                                continue\n",
    "                        gt = np.loadtxt(os.path.join(gt_root, shp+'.normals'))\n",
    "                        # print(os.path.join(pred_root, shp+'.normals'), os.path.join(gt_root, shp+'.normals'))\n",
    "                        rms_list.append(rms_angular_error(torch.tensor(pred), torch.tensor(gt)))\n",
    "        print(testset)\n",
    "        print(ind+1, \"total pcds\")\n",
    "        print(np.array(rms_list).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_no_noise.txt\n",
      "19 total pcds\n",
      "5.578300624693992\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_low_noise.txt\n",
      "19 total pcds\n",
      "9.541549662429185\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_med_noise.txt\n",
      "19 total pcds\n",
      "14.42995104969164\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_high_noise.txt\n",
      "19 total pcds\n",
      "nan\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_gradient.txt\n",
      "16 total pcds\n",
      "nan\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_striped.txt\n",
      "16 total pcds\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_541302/49119685.py:52: RuntimeWarning: Mean of empty slice.\n",
      "  print(np.array(rms_list).mean())\n"
     ]
    }
   ],
   "source": [
    "'''final wala'''\n",
    "import os\n",
    "from natsort import natsorted\n",
    "\n",
    "def rms_angular_error(estimated_normals, ground_truth_normals):\n",
    "    estimated_normals = F.normalize(estimated_normals, dim=1)\n",
    "    ground_truth_normals = F.normalize(ground_truth_normals, dim=1)\n",
    "\n",
    "    dot_product = torch.sum(estimated_normals * ground_truth_normals, dim=1)\n",
    "    dot_product = torch.clamp(dot_product, -1.0, 1.0)\n",
    "    dot_product = torch.abs(dot_product)\n",
    "    angular_diff = torch.acos(dot_product) * torch.div(180.0, torch.pi)\n",
    "    squared_diff = angular_diff.pow(2)\n",
    "    mean_squared_diff = torch.mean(squared_diff)\n",
    "    rms_angular_error = torch.sqrt(mean_squared_diff)\n",
    "    return rms_angular_error.item() \n",
    "\n",
    "pred_root = 'graphfit_deg/GraphFit/results_599'\n",
    "gt_root = '/media/ashish/zoneD/AdaFit/data/pcpnet'\n",
    "\n",
    "test_Sets = ['/media/ashish/zoneD/AdaFit/data/pcpnet/testset_no_noise.txt', '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_low_noise.txt', \n",
    "             '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_med_noise.txt','/media/ashish/zoneD/AdaFit/data/pcpnet/testset_high_noise.txt', \n",
    "             '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_gradient.txt', '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_striped.txt']\n",
    "\n",
    "for testset in test_Sets:\n",
    "        rms_mean = 0\n",
    "        rms_list = []\n",
    "        shapes = []\n",
    "        #read files in pred_root\n",
    "\n",
    "        with open(testset, 'r') as f: \n",
    "                file_content = f.read()\n",
    "                words = file_content.split()\n",
    "                shapes.extend(words)\n",
    "                \n",
    "\n",
    "        for ind,shp in enumerate(shapes[:]):\n",
    "                #check if file exists\n",
    "                if (os.path.exists(os.path.join(pred_root, shp+'.normals')) and os.path.exists(os.path.join(gt_root, shp+'.normals'))):\n",
    "                        # print(os.path.join(pred_root, shp+'.normals'))\n",
    "                        try:\n",
    "                                pred = np.loadtxt(os.path.join(pred_root, shp+'.normals'), encoding='utf-8')\n",
    "                                # Process the loaded data here\n",
    "                        except UnicodeDecodeError:\n",
    "                                print(f\"Skipping file '{os.path.join(pred_root, shp+'.normals')}' due to UnicodeDecodeError.\")\n",
    "                                continue\n",
    "                        gt = np.loadtxt(os.path.join(gt_root, shp+'.normals'))\n",
    "                        # print(os.path.join(pred_root, shp+'.normals'), os.path.join(gt_root, shp+'.normals'))\n",
    "                        rms_list.append(rms_angular_error(torch.tensor(pred), torch.tensor(gt)))\n",
    "        print(testset)\n",
    "        print(ind+1, \"total pcds\")\n",
    "        print(np.array(rms_list).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_no_noise.txt\n",
      "19 total pcds\n",
      "5.608473917390834\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_low_noise.txt\n",
      "19 total pcds\n",
      "8.818107656163843\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_med_noise.txt\n",
      "19 total pcds\n",
      "nan\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_high_noise.txt\n",
      "19 total pcds\n",
      "nan\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_gradient.txt\n",
      "16 total pcds\n",
      "nan\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_striped.txt\n",
      "16 total pcds\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_541302/1666842374.py:52: RuntimeWarning: Mean of empty slice.\n",
      "  print(np.array(rms_list).mean())\n"
     ]
    }
   ],
   "source": [
    "'''final wala'''\n",
    "import os\n",
    "from natsort import natsorted\n",
    "\n",
    "def rms_angular_error(estimated_normals, ground_truth_normals):\n",
    "    estimated_normals = F.normalize(estimated_normals, dim=1)\n",
    "    ground_truth_normals = F.normalize(ground_truth_normals, dim=1)\n",
    "\n",
    "    dot_product = torch.sum(estimated_normals * ground_truth_normals, dim=1)\n",
    "    dot_product = torch.clamp(dot_product, -1.0, 1.0)\n",
    "    dot_product = torch.abs(dot_product)\n",
    "    angular_diff = torch.acos(dot_product) * torch.div(180.0, torch.pi)\n",
    "    squared_diff = angular_diff.pow(2)\n",
    "    mean_squared_diff = torch.mean(squared_diff)\n",
    "    rms_angular_error = torch.sqrt(mean_squared_diff)\n",
    "    return rms_angular_error.item() \n",
    "\n",
    "pred_root = 'modi_sa_nlearn_7/GraphFit/results_599'\n",
    "gt_root = '/media/ashish/zoneD/AdaFit/data/pcpnet'\n",
    "\n",
    "test_Sets = ['/media/ashish/zoneD/AdaFit/data/pcpnet/testset_no_noise.txt', '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_low_noise.txt', \n",
    "             '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_med_noise.txt','/media/ashish/zoneD/AdaFit/data/pcpnet/testset_high_noise.txt', \n",
    "             '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_gradient.txt', '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_striped.txt']\n",
    "\n",
    "for testset in test_Sets:\n",
    "        rms_mean = 0\n",
    "        rms_list = []\n",
    "        shapes = []\n",
    "        #read files in pred_root\n",
    "\n",
    "        with open(testset, 'r') as f: \n",
    "                file_content = f.read()\n",
    "                words = file_content.split()\n",
    "                shapes.extend(words)\n",
    "                \n",
    "\n",
    "        for ind,shp in enumerate(shapes[:]):\n",
    "                #check if file exists\n",
    "                if (os.path.exists(os.path.join(pred_root, shp+'.normals')) and os.path.exists(os.path.join(gt_root, shp+'.normals'))):\n",
    "                        # print(os.path.join(pred_root, shp+'.normals'))\n",
    "                        try:\n",
    "                                pred = np.loadtxt(os.path.join(pred_root, shp+'.normals'), encoding='utf-8')\n",
    "                                # Process the loaded data here\n",
    "                        except UnicodeDecodeError:\n",
    "                                print(f\"Skipping file '{os.path.join(pred_root, shp+'.normals')}' due to UnicodeDecodeError.\")\n",
    "                                continue\n",
    "                        gt = np.loadtxt(os.path.join(gt_root, shp+'.normals'))\n",
    "                        # print(os.path.join(pred_root, shp+'.normals'), os.path.join(gt_root, shp+'.normals'))\n",
    "                        rms_list.append(rms_angular_error(torch.tensor(pred), torch.tensor(gt)))\n",
    "        print(testset)\n",
    "        print(ind+1, \"total pcds\")\n",
    "        print(np.array(rms_list).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_no_noise.txt\n",
      "19 total pcds\n",
      "4.3997121658101745\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_low_noise.txt\n",
      "19 total pcds\n",
      "9.067822671414335\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_med_noise.txt\n",
      "19 total pcds\n",
      "17.42656059074695\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_high_noise.txt\n",
      "19 total pcds\n",
      "25.471542266631065\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_gradient.txt\n",
      "16 total pcds\n",
      "4.944777767693807\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_striped.txt\n",
      "16 total pcds\n",
      "5.037272965788798\n"
     ]
    }
   ],
   "source": [
    "'''final wala'''\n",
    "import os\n",
    "from natsort import natsorted\n",
    "\n",
    "def rms_angular_error(estimated_normals, ground_truth_normals):\n",
    "    estimated_normals = F.normalize(estimated_normals, dim=1)\n",
    "    ground_truth_normals = F.normalize(ground_truth_normals, dim=1)\n",
    "\n",
    "    dot_product = torch.sum(estimated_normals * ground_truth_normals, dim=1)\n",
    "    dot_product = torch.clamp(dot_product, -1.0, 1.0)\n",
    "    dot_product = torch.abs(dot_product)\n",
    "    angular_diff = torch.acos(dot_product) * torch.div(180.0, torch.pi)\n",
    "    squared_diff = angular_diff.pow(2)\n",
    "    mean_squared_diff = torch.mean(squared_diff)\n",
    "    rms_angular_error = torch.sqrt(mean_squared_diff)\n",
    "    return rms_angular_error.item() \n",
    "\n",
    "pred_root = 'original_graphfit/GraphFit/results_599'\n",
    "gt_root = '/media/ashish/zoneD/AdaFit/data/pcpnet'\n",
    "\n",
    "test_Sets = ['/media/ashish/zoneD/AdaFit/data/pcpnet/testset_no_noise.txt', '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_low_noise.txt', \n",
    "             '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_med_noise.txt','/media/ashish/zoneD/AdaFit/data/pcpnet/testset_high_noise.txt', \n",
    "             '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_gradient.txt', '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_striped.txt']\n",
    "\n",
    "for testset in test_Sets:\n",
    "        rms_mean = 0\n",
    "        rms_list = []\n",
    "        shapes = []\n",
    "        #read files in pred_root\n",
    "\n",
    "        with open(testset, 'r') as f: \n",
    "                file_content = f.read()\n",
    "                words = file_content.split()\n",
    "                shapes.extend(words)\n",
    "                \n",
    "\n",
    "        for ind,shp in enumerate(shapes[:]):\n",
    "                #check if file exists\n",
    "                if (os.path.exists(os.path.join(pred_root, shp+'.normals')) and os.path.exists(os.path.join(gt_root, shp+'.normals'))):\n",
    "                        # print(os.path.join(pred_root, shp+'.normals'))\n",
    "                        try:\n",
    "                                pred = np.loadtxt(os.path.join(pred_root, shp+'.normals'), encoding='utf-8')\n",
    "                                # Process the loaded data here\n",
    "                        except UnicodeDecodeError:\n",
    "                                print(f\"Skipping file '{os.path.join(pred_root, shp+'.normals')}' due to UnicodeDecodeError.\")\n",
    "                                continue\n",
    "                        gt = np.loadtxt(os.path.join(gt_root, shp+'.normals'))\n",
    "                        # print(os.path.join(pred_root, shp+'.normals'), os.path.join(gt_root, shp+'.normals'))\n",
    "                        rms_list.append(rms_angular_error(torch.tensor(pred), torch.tensor(gt)))\n",
    "        print(testset)\n",
    "        print(ind+1, \"total pcds\")\n",
    "        print(np.array(rms_list).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_no_noise.txt\n",
      "19 total pcds\n",
      "4.720055298457488\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_low_noise.txt\n",
      "19 total pcds\n",
      "12.235197576328181\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_med_noise.txt\n",
      "19 total pcds\n",
      "nan\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_high_noise.txt\n",
      "19 total pcds\n",
      "nan\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_gradient.txt\n",
      "16 total pcds\n",
      "nan\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_striped.txt\n",
      "16 total pcds\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2600616/421108214.py:52: RuntimeWarning: Mean of empty slice.\n",
      "  print(np.array(rms_list).mean())\n"
     ]
    }
   ],
   "source": [
    "'''final wala'''\n",
    "import os\n",
    "from natsort import natsorted\n",
    "\n",
    "def rms_angular_error(estimated_normals, ground_truth_normals):\n",
    "    estimated_normals = F.normalize(estimated_normals, dim=1)\n",
    "    ground_truth_normals = F.normalize(ground_truth_normals, dim=1)\n",
    "\n",
    "    dot_product = torch.sum(estimated_normals * ground_truth_normals, dim=1)\n",
    "    dot_product = torch.clamp(dot_product, -1.0, 1.0)\n",
    "    dot_product = torch.abs(dot_product)\n",
    "    angular_diff = torch.acos(dot_product) * torch.div(180.0, torch.pi)\n",
    "    squared_diff = angular_diff.pow(2)\n",
    "    mean_squared_diff = torch.mean(squared_diff)\n",
    "    rms_angular_error = torch.sqrt(mean_squared_diff)\n",
    "    return rms_angular_error.item() \n",
    "\n",
    "pred_root = 'log/GraphFit/results_599'\n",
    "gt_root = '/media/ashish/zoneD/AdaFit/data/pcpnet'\n",
    "\n",
    "test_Sets = ['/media/ashish/zoneD/AdaFit/data/pcpnet/testset_no_noise.txt', '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_low_noise.txt', \n",
    "             '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_med_noise.txt','/media/ashish/zoneD/AdaFit/data/pcpnet/testset_high_noise.txt', \n",
    "             '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_gradient.txt', '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_striped.txt']\n",
    "\n",
    "for testset in test_Sets:\n",
    "        rms_mean = 0\n",
    "        rms_list = []\n",
    "        shapes = []\n",
    "        #read files in pred_root\n",
    "\n",
    "        with open(testset, 'r') as f: \n",
    "                file_content = f.read()\n",
    "                words = file_content.split()\n",
    "                shapes.extend(words)\n",
    "                \n",
    "\n",
    "        for ind,shp in enumerate(shapes[:]):\n",
    "                #check if file exists\n",
    "                if (os.path.exists(os.path.join(pred_root, shp+'.normals')) and os.path.exists(os.path.join(gt_root, shp+'.normals'))):\n",
    "                        # print(os.path.join(pred_root, shp+'.normals'))\n",
    "                        try:\n",
    "                                pred = np.loadtxt(os.path.join(pred_root, shp+'.normals'), encoding='utf-8')\n",
    "                                # Process the loaded data here\n",
    "                        except UnicodeDecodeError:\n",
    "                                print(f\"Skipping file '{os.path.join(pred_root, shp+'.normals')}' due to UnicodeDecodeError.\")\n",
    "                                continue\n",
    "                        gt = np.loadtxt(os.path.join(gt_root, shp+'.normals'))\n",
    "                        # print(os.path.join(pred_root, shp+'.normals'), os.path.join(gt_root, shp+'.normals'))\n",
    "                        rms_list.append(rms_angular_error(torch.tensor(pred), torch.tensor(gt)))\n",
    "        print(testset)\n",
    "        print(ind+1, \"total pcds\")\n",
    "        print(np.array(rms_list).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_no_noise.txt\n",
      "19 total pcds\n",
      "4.61044504235487\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_low_noise.txt\n",
      "19 total pcds\n",
      "12.34819561602133\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_med_noise.txt\n",
      "19 total pcds\n",
      "nan\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_high_noise.txt\n",
      "19 total pcds\n",
      "nan\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_gradient.txt\n",
      "16 total pcds\n",
      "nan\n",
      "/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_striped.txt\n",
      "16 total pcds\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2600616/1180887642.py:52: RuntimeWarning: Mean of empty slice.\n",
      "  print(np.array(rms_list).mean())\n"
     ]
    }
   ],
   "source": [
    "'''final wala'''\n",
    "import os\n",
    "from natsort import natsorted\n",
    "\n",
    "def rms_angular_error(estimated_normals, ground_truth_normals):\n",
    "    estimated_normals = F.normalize(estimated_normals, dim=1)\n",
    "    ground_truth_normals = F.normalize(ground_truth_normals, dim=1)\n",
    "\n",
    "    dot_product = torch.sum(estimated_normals * ground_truth_normals, dim=1)\n",
    "    dot_product = torch.clamp(dot_product, -1.0, 1.0)\n",
    "    dot_product = torch.abs(dot_product)\n",
    "    angular_diff = torch.acos(dot_product) * torch.div(180.0, torch.pi)\n",
    "    squared_diff = angular_diff.pow(2)\n",
    "    mean_squared_diff = torch.mean(squared_diff)\n",
    "    rms_angular_error = torch.sqrt(mean_squared_diff)\n",
    "    return rms_angular_error.item() \n",
    "\n",
    "pred_root = 'modi_sa_nlearn/GraphFit/results_599'\n",
    "gt_root = '/media/ashish/zoneD/AdaFit/data/pcpnet'\n",
    "\n",
    "test_Sets = ['/media/ashish/zoneD/AdaFit/data/pcpnet/testset_no_noise.txt', '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_low_noise.txt', \n",
    "             '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_med_noise.txt','/media/ashish/zoneD/AdaFit/data/pcpnet/testset_high_noise.txt', \n",
    "             '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_gradient.txt', '/media/ashish/zoneD/AdaFit/data/pcpnet/testset_vardensity_striped.txt']\n",
    "\n",
    "for testset in test_Sets:\n",
    "        rms_mean = 0\n",
    "        rms_list = []\n",
    "        shapes = []\n",
    "        #read files in pred_root\n",
    "\n",
    "        with open(testset, 'r') as f: \n",
    "                file_content = f.read()\n",
    "                words = file_content.split()\n",
    "                shapes.extend(words)\n",
    "                \n",
    "\n",
    "        for ind,shp in enumerate(shapes[:]):\n",
    "                #check if file exists\n",
    "                if (os.path.exists(os.path.join(pred_root, shp+'.normals')) and os.path.exists(os.path.join(gt_root, shp+'.normals'))):\n",
    "                        # print(os.path.join(pred_root, shp+'.normals'))\n",
    "                        try:\n",
    "                                pred = np.loadtxt(os.path.join(pred_root, shp+'.normals'), encoding='utf-8')\n",
    "                                # Process the loaded data here\n",
    "                        except UnicodeDecodeError:\n",
    "                                print(f\"Skipping file '{os.path.join(pred_root, shp+'.normals')}' due to UnicodeDecodeError.\")\n",
    "                                continue\n",
    "                        gt = np.loadtxt(os.path.join(gt_root, shp+'.normals'))\n",
    "                        # print(os.path.join(pred_root, shp+'.normals'), os.path.join(gt_root, shp+'.normals'))\n",
    "                        rms_list.append(rms_angular_error(torch.tensor(pred), torch.tensor(gt)))\n",
    "        print(testset)\n",
    "        print(ind+1, \"total pcds\")\n",
    "        print(np.array(rms_list).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(x, k):\n",
    "    inner = -2 * torch.matmul(x.transpose(2, 1), x)\n",
    "    xx = torch.sum(x ** 2, dim=1, keepdim=True)\n",
    "    pairwise_distance = -xx - inner - xx.transpose(2, 1)\n",
    "\n",
    "    idx = pairwise_distance.topk(k=k, dim=-1)[1]  # (batch_size, num_points, k)\n",
    "    return idx\n",
    "def get_graph_feature(x, points, k=20, idx=None, dim9=False):\n",
    "    # pdb.set_trace()\n",
    "    print(x.shape)\n",
    "    batch_size = x.size(0)\n",
    "    num_points = x.size(2)\n",
    "    x = x.view(batch_size, -1, num_points)\n",
    "    if idx is None:\n",
    "        if dim9 == False:\n",
    "            idx = knn(x, k=k)  # (batch_size, num_points, k)\n",
    "        else:\n",
    "            idx = knn(x[:, 6:], k=k)\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "    idx_base = torch.arange(0, batch_size, device=device).view(-1, 1, 1) * num_points\n",
    "    # print(idx_base)\n",
    "\n",
    "    idx = idx + idx_base\n",
    "    # print(idx)\n",
    "\n",
    "    idx = idx.view(-1)\n",
    "\n",
    "    _, num_dims, _ = x.size()\n",
    "\n",
    "    x = x.transpose(2,1).contiguous()  # (batch_size, num_points, num_dims)  -> (batch_size*num_points, num_dims) #   batch_size * num_points * k + range(0, batch_size*num_points)\n",
    "    feature = x.view(batch_size * num_points, -1)[idx, :]\n",
    "    feature = feature.view(batch_size, num_points, k, num_dims)\n",
    "    print(x.shape)\n",
    "    x = x.view(batch_size, num_points, 1, num_dims).repeat(1, 1, k, 1)\n",
    "    print(feature.shape, x.shape)\n",
    "\n",
    "    points = points.transpose(2, 1).contiguous()\n",
    "    points2 = points.view(batch_size*num_points, -1)[idx, :]\n",
    "    print(points.shape)\n",
    "    print(points2.shape)\n",
    "    points2 = points2.view(batch_size, num_points, k, 3)\n",
    "    print(points2.shape)\n",
    "    points = points.view(batch_size, num_points, 1, 3).repeat(1, 1, k, 1)\n",
    "    feature = torch.cat((feature - x, x, points2-points, points, points2), dim=3).permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "    # pdb.set_trace()\n",
    "\n",
    "    return feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 9, 2048])\n",
      "torch.Size([2, 2048, 9])\n",
      "torch.Size([2, 2048, 20, 9]) torch.Size([2, 2048, 20, 9])\n",
      "torch.Size([2, 2048, 3])\n",
      "torch.Size([81920, 3])\n",
      "torch.Size([2, 2048, 20, 3])\n"
     ]
    }
   ],
   "source": [
    "t = torch.rand(2, 9, 2048).cuda()\n",
    "points = torch.rand(2, 3, 2048).cuda()\n",
    "ans = get_graph_feature(t, points, k=20, dim9=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 27, 2048, 20])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
